---
title: "A partial reproduction of \"Bayesian regularised quantile regression\""
authors: "Lucie Tournier & Arthur Katossky"
date: "December 2022"
date-format: "MMMM, YYYY"
bibliography: quantile-regression.bib
format:
  pdf:
    pdf-engine: pdflatex
    include-in-header:
      text: |
        \usepackage{amsmath}
        \usepackage{bbm}
        \usepackage{xcolor}
        \usepackage{cancel}
#    classoption:
#      - twocolumn
---

This reports reproduces the results from @2010-quantile-regression, focussing on their extension of the quantile regression with Elasticnet regularisation in a Bayesian framework.

## Introduction

Regression is the idea that, for an outcome $y\in\mathbb{R}$ and associated inputs $\mathbf{x}\in\mathbb{R}^d$ ($d \in \mathbb{N}_\star$), we may suppose that in some sense $y\simeq \mathbf{x}^\top\boldsymbol{\beta}$, for some real vector $\boldsymbol{\beta}\in\mathbb{R}^d$.
Given a set of observations $(y_i,\mathbf{x}_i)_{i=1..n}$, classical regression consists in estimating the unknown parameter $\boldsymbol{\beta}$ by setting:
$$\hat{\boldsymbol{\beta}}_n=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^d}\sum_{i=1}^n \ell(y_i, \mathbf{x}_i^\top\boldsymbol{\beta})$${#eq-classical-regression}
... where $\ell$ is some loss function, typically $\ell_2(a,b)=(a-b)^2$ for the ordinary least-squares regression.
In a **quantile regression**, one rather opts for the asymmetric "check loss" : $$\ell_\theta(a,b)=\left\{\begin{array}{cl}\theta (a-b), & \text { if } a \geqslant b \\ -(1-\theta) (a-b), & \text { if }  a<b\end{array}\right.$$ ... with $\theta\in(0,1)$, which generalises absolute loss $\ell_0(a,b)=|a-b|$ leading to so-called median regression[^1].

[^1]: Note that $\ell_0$ is proportional to $\ell_\theta$ with $\theta=0.5$ and that the proportionality coefficient does not matter in @eq-classical-regression, so the two losses are completely interchangeable.

Finally, Elasticnet regularisation consists into constraining the values of $\boldsymbol{\beta}$ not to move too far away from the origin, in the sense that both $\|\boldsymbol{\beta}\|_1$ (the taxicab distance from the origin) and $\|\boldsymbol{\beta}\|_2^2$ (the square of the Euclidean distance from the origin) are small.
This leads to the final problem:
$$
\hat{\boldsymbol{\beta}}_n(\lambda_1,\lambda_2,\theta) = \arg\min_{\boldsymbol{\beta}\in\mathbb{R}^d}\sum_{i=1}^n \ell_\theta(y_i, \mathbf{x}_i^\top\boldsymbol{\beta})+\lambda_1\|\boldsymbol{\beta}\|_1+\lambda_2\|\boldsymbol{\beta}\|^2_2
$$ {#eq-classical-elasticnet}
... where $\lambda_1$ and $\lambda_2$ control how stringent each regularisation is (less strigent as $\lambda \to 0$).

The article studies **how to put Elasticnet quantile regression into a Bayesian paradigm**, thus allowing for priors over the distributions of both $\boldsymbol{\beta}$ and the approximation error in $y\simeq \mathbf{x}^\top\boldsymbol{\beta}$.
The main advantages of such a Bayesian paradigm is to obtain exact distribution for the target estimator $\hat{\boldsymbol{\beta}}_n|\theta$, even for small sample sizes.
The authors claim that their results incidentally "provide more accurate estimates and better prediction accuracy than their non-Bayesian peers" but the evidence they submit is more nuanced.

## Putting regularised quantile regression (Elasticnet) into the Bayesian formalism

How can we reinterpret @eq-classical-elasticnet in a Bayesian framework? First notice that:
$$
\arg \min \sum_{i=1}^n \ell_\theta(y_i, \mathbf{x}_i^\top\boldsymbol{\beta}) 
= \arg \max \prod_{i=1}^n \exp (-\ell_\theta(y_i, \mathbf{x}_i^\top\boldsymbol{\beta}))
$$
The (stricly decreasing) exponential-negative transform allows us to re-interpret error minimisation as the maximisation of some joint probability over independent observations, each under some exponential probabilistic distribution.
The question becomes how to choose the priors so that the posterior distribution has this required form.

Let's define the prediction error $u\overset{\Delta}=y-\mathbf{x}^\top\boldsymbol{\beta}$ and let suppose that $u$ follows a (centred) skewed Laplace distribution with parameters $(0,\tau,\theta)$.
That is, with $\tau>0$ and $\theta>0$ and $\forall u \in \mathbb{R}$: $$\pi(u|\tau) = \theta(1 - \theta) \tau \exp(-\tau \ell_\theta(u)).$$
Then we get the following joint posterior on the sample $(y_i,\mathbf{x}_i)_{i=1..n}$:
$$
f(\boldsymbol{\beta}\mid y_1...y_n,\mathbf{x}_1...\mathbf{x}_n ; \tau ; \theta)
= \theta^n (1 - \theta)^n \tau^n \exp\bigg\{-\tau \sum_{i=1}^n \ell_\theta(y_i - \mathbf{x}_i^\top \boldsymbol{\beta} ) \bigg\}
$$
Maximizing this expression is equivalent to @eq-classical-elasticnet without the penalties since we can discard all the (strictly positive) multiplicative constants in front and inside of the exponential.

What about the two penalty terms $\lambda_1\|\boldsymbol{\beta}\|_1$ and $\lambda_2\|\boldsymbol{\beta}\|^2_2$? Setting a prior on $\boldsymbol{\beta}$ of the form:
$$
\pi(\boldsymbol{\beta} | \eta_1,\eta_2)
\propto \exp \Big\{-\eta_1\|\boldsymbol{\beta}\|_1-\eta_2\|\boldsymbol{\beta}\|^2_2 \Big\}
$${#eq-beta-prior}
... leads to the posterior distribution:
$$
\pi(\boldsymbol{\beta} |y_1...y_n, \mathbf{x}_1 ... \mathbf{x}_n ; \eta_1,\eta_2,\tau ; \theta)
\propto { \exp \Big\{ - \tau \sum_{i=1}^n \ell_\theta(y_i - x_i^\top\boldsymbol{\beta})
-\eta_1\|\boldsymbol{\beta}\|_1-\eta_2\|\boldsymbol{\beta}\|^2_2 \Big\} }
$$
.. which is almost what we where looking for! If we set $\lambda_\bullet\overset{\Delta}=\frac{\eta_\bullet}{\tau}$, we get $\eta_\bullet=\lambda_\bullet\tau$ and we can factorise $\tau$ inside the exponential. As $\tau>0$, maximising this expression is exactly equivalent to @eq-classical-elasticnet.

These calculations give us, under the assumption of the skewed Laplace distribution for the residuals, an explicit expression for the distribution of the estimator $\hat{\boldsymbol{\beta}}_n\overset{\Delta}=\boldsymbol{\beta}\mid y_1...y_n, \mathbf{x}_1 ... \mathbf{x}_n$ in the quantile regression with elastic-net regularisation *conditionnally on* the quantile $\theta$ and the regularisation parameters $\lambda_1$ and $\lambda_2$ (or $\eta_1$, $\eta_2$ and $\tau$).

Notice that there is a fundamental asymmetry in the parameters. The $\theta \in (0,1)$ parameter is fixed in advanced and will always stay like that : it is the level of the quantile regression. Thus we will always reason "conditionally to $\theta$". However, this is not the case for penalty coefficients $\lambda_1$ and $\lambda_2$ (or the three coefficients $\eta_1, \eta_2, \tau$). In a frequentist context we generally think of the penalty as fixed, but more often than not in practice we actually consider that different types of data need different amount of regularisation and we "fine-tune" these coefficients to achieve minimal error on hold-out samples. Thus, $\lambda_1$ and $\lambda_2$ may be thought of as true Bayesian parameters whose distribution will get more precise when confronted to data.

This has two consequences:

1.  we need to specify a *prior* distribution for $(\lambda_1,\lambda_2)$ or $(\tau, \eta_1, \eta_2)$
2.  for many such distributions, the (unconditional) *posterior* distribution of $\boldsymbol{\hat{\beta}}_n\mid \theta$ will be computationally intractable

We thus need to resort to computational methods to sort out the posterior unconditional distribution, and the Gibbs algorithm seems perfectly suited for this as we readily have a closed form for the conditional $\boldsymbol{\beta} |y_1...y_n, \mathbf{x}_1 ... \mathbf{x}_n ; \eta_1,\eta_2,\tau ; \theta$.

## Distribution of interest parameter $\boldsymbol{\beta}$ may be elicited through Gibbs sampling

Specifying the full Bayesian model is far from done, mainly for tractibility reasons. Alas, the authors rarely discuss what the tractability issued precisely are, so all we can do is to follow their lead. First, the authors show that $u\sim \text{Laplace}(0,\tau,\theta)$ can be re-written as : $\tau u=\xi_1 v+\xi_2 \, \sqrt{v}\; z$ where $v\sim\mathcal{E}(1)$ and $z\sim\mathcal{N}(0,1)$, and where $\xi_1=\frac{1-2 \theta}{\theta(1-\theta)}$ and $\xi_2=\sqrt{\frac{2}{\theta(1-\theta)}}$. Now writing $\tilde v\overset{\Delta}=v/\tau$, we get both $\tilde v\mid \tau\sim\mathcal{E}(1/\tau)$ and: $$u\overset{\Delta}=\xi_1\tilde v+(\xi_2/\sqrt{\tau})\;\sqrt{\tilde{v}} \;z \mid \tau \sim \mathrm{Laplace}(0,\tau,\theta)$${#eq-u-prior}

Then they show that the following full hierarchical model both guarantees tractablity and achieved the desired priors on $u\mid \tau$ (@eq-u-prior) and $\boldsymbol{\beta}\mid\eta_1,\eta_2$ (@eq-beta-prior) [^2]:
$$
\begin{aligned}
\tilde{\eta}_1 & \sim && \mathrm{Gamma}(c_1,d_1) &
\tau & \sim && \mathrm{Gamma}(a,b) \\
\eta_2 & \sim && \mathrm{Gamma}(c_2,d_2) &
\tilde v \mid \tau & \sim && \mathcal{E}(1/\tau) \\
t_k | \tilde{\eta}_1 & \overset{\perp\!\!\!\perp}\sim && 
\begin{array}{l}
\Gamma^{-1}(1/2, \tilde{\eta}_1) \; \tilde{\eta}_1^{-1/2} \\
\times t_k^{-1/2} \exp(-\tilde{\eta}_1t_k) \mathbbm{1}(t_k>1)
\end{array} &
z & \sim && \mathcal{N}(0,1) \\
\beta_k | t_k, \eta_2 & \overset{\perp\!\!\!\perp}\sim &&
\begin{array}{l}
\frac{1}{\sqrt{2\pi(t_k - 1)/(2\eta_2t_k)}} \\
\times \exp\left\{ -\frac{1}{2} \left( \frac{t_k-1}{2\eta_2t_k}\right)^{-1} \beta_k^2 \right\}
\end{array}
\end{aligned}
$$

[^2]: For reasons just stated, $\theta$ is now omitted in the conditioning and considered a constant.

... where $a,b,c_1,c_2,d_2,d_2$ are strictly positive constants arbitrarily chosen to express the researcher's prior[^4] and $\Gamma(\cdot,\cdot)$ is the upper incomplete gamma function. The random latent vector $\mathbf{t}\overset{\Delta}=(t_k)_{k=1..d}$ is introduced for analytical reasons whereas $\tilde{\eta}_1 \overset{\Delta}=\eta_1^2 /\left(4 \eta_2\right)$ arises from the computation of the constant in @eq-beta-prior.

[^4]: If $a,b,c_1,c_2,d_2,d_2$ are all zero, then the prior is non-informative.

But now for estimating $\beta$ on a sample, we will use a Gibbs sampler for which we need the conditional posteriors. Noting $\mathbf{y}=(y_1..y_n)$, $\mathbf{X}=(\mathbf{x}_1..\mathbf{x}_n)$, $\mathbf{t}_{-k}=(t_1..t_{k-1},t_{k+1},..t_d)$ and following similar conventions for $\boldsymbol{\beta}_{-k}$ and $\tilde{\mathbf{v}}_{-i}$, they are given by:
$$
\begin{aligned}
\tilde{\eta}_1 & \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}, \boldsymbol{\beta}, \mathbf{t}, \tau, \eta_2& \propto & \Gamma^{-p}\left(1 / 2, \tilde{\eta}_1\right) \tilde{\eta}_1^{p / 2+\textcolor{red}{c_1}-1} \exp \left\{-\tilde{\eta}_1\left[\textcolor{red}{d_1}+\sum_{k=1}^p t_k\right]\right\} \\
\eta_2 & \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}, \boldsymbol{\beta}, \mathbf{t}, \tau, \tilde{\eta}_1 & \sim & \mathrm{Gamma}(\textcolor{red}{c_2}+\frac{p}{2},\textcolor{red}{d_2}+\sum_{k=1}^p \frac{t_k}{t_k-1} \beta_k^2 ) \\
t_k-1 & \mid \mathbf{y}, \mathbf{X}, \tilde{\mathbf{v}}, \boldsymbol{\beta}, \mathbf{t}_{-k}, \tau, \tilde{\eta}_1, \eta_2 & \sim & \mathrm{Gen. inv. Gaussian}\left(\frac{1}{2}, \; 2\tilde\eta_1, \; 2\eta_2\beta_k^2\right) \\
\beta_k & \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}, \boldsymbol{\beta}_{-k}, \mathbf{t}, \tau, \tilde{\eta}_1, \eta_2 & \sim & \mathcal{N}\left(
 \frac{\tau \tilde{\sigma}_k^2}{\textcolor{red}{\xi_2}^2} \sum_{i=1}^n \frac{\tilde{y}_{i,-k} x_{i k}}{\tilde{v}_i}, \; \tilde{\sigma}_k^2
\right) \\
\tau & \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}, \boldsymbol{\beta}, \mathbf{t}, \tilde{\eta}_1, \eta_2 & \sim & \mathrm{Gamma}\left(\textcolor{red}{a} +\frac{3n}{2},\textcolor{red}{b}+\sum_{i=1}^n\left(\frac{\tilde y_i^2}{2 \textcolor{red}{\xi_2}^2 \tilde{v}_i}+\tilde{v}_i\right)\right) \\
\tilde{v}_i & \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}_{-i}, \boldsymbol{\beta}, \mathbf{t}, \tau, \tilde{\eta}_1, \eta_2 & \sim & \mathrm{Gen. inv. Gaussian}\left(\frac{1}{2},\frac{\tau \textcolor{red}{\xi_1}^2}{\textcolor{red}{\xi_2}^2}+2 \tau,\frac{\tau\left(y_i-\mathbf{x}_i^\top \boldsymbol{\beta}\right)^2}{\textcolor{red}{\xi_2}^2}\right)
\end{aligned}
$$

... where red denotes constants from the priors, and:
$$
\begin{aligned}
\tilde y & = y-\;\;\;\;\mathbf{x}^\top \boldsymbol{\beta}\;\;\:-\textcolor{red}{\xi_1} \tilde{v} \\
\tilde y_{-k} & = y-{\mathbf{x}_{-k}}^\top \boldsymbol{\beta}_{-k}-\textcolor{red}{\xi_1} \tilde{v} \\
\tilde \sigma_k^2 & = \frac{1}{\frac{\tau}{\textcolor{red}{\xi_2}^{2}} \sum_{i=1}^n \frac{x_{i k}^2}{ \tilde{v}_i}+2 \eta_2 \frac{t_k}{t_k-1}}\\
\end{aligned}
$$

The authors judge difficult to directly sample from the distribution $\tilde{\eta}_1 \mid \mathbf{X}, \mathbf{y}, \tilde{\mathbf{v}}, \boldsymbol{\beta}, \mathbf{t}, \tau, \eta_2$ since (a) it does not correspond to any known distribution, (b) we know the distribution only up to a multiplicative constant and (c) there is a $\eta_1$ inside the $\Gamma(\cdot,\cdot)$ function which hinders tractability. They thus resort to one step of Metropolis-Hastings, which has the nice property of not needing the missing multiplicative constant[^3].

With this last step, we can now run the Gibbs algorithm in full, thus estimate the unconditionnal distribution for $\boldsymbol{\beta}$ and turn to simulations.

[^3]: Instead of sampling $\tilde \eta_1$ directly from its conditional law, we first sample a candidate $\eta_c$ from the distribution $\textrm{Gamma}(p+c_1, d_1 + \sum_k (t_k)$ (the authors use $\textrm{Gamma}(p+c_1, d_1 + \sum_k (t_k-1)$ but we experimentally found that results were more stable with our choice) which we accept with probability $\max(1, \alpha)$, where $\alpha$ is the ration of the density function of the Gamma distribution, evaluated in the candidate and in the current value of $\tilde \eta_1$ :
$g(\eta_c)/g(\tilde \eta_1)$.
If the candidate is rejected, the value of $\tilde \eta_1$ remains unchanged.

## Simulation

### Tuning of the Gibbs algorithm

With the Gibbs algorithm, as with all MCMC methods, there is a burn-in phase (during which the Markov chain converges to its stationary law) and dependence between successive iterations of the algorithm. We thus first check for the length of the burn-in period, then for correlation between successive runs. We find that 20 rounds at the beginning then 20 steps between each draws are enough to provide satisfactory results.

```{r}
#| echo: false
library('statmod')
library('GIGrvg')
library('mvtnorm')
library('hqreg')
```

```{r, data-generation-function}
#| echo: false

Xi1 <- function(theta){ return ( (1 - 2*theta) / (theta* (1-theta)) ) }
Xi2 <- function(theta){ return (sqrt(2 / (theta * (1-theta)))) }

data_generation <- function(n, theta, beta0=c(3, 3, 0, 3, 3), tau0=2, g_err=TRUE){
  # renvoie les un tableau dont la première colonne y,
  # les colonnes suivantes x \# n nombre d échantillons à générer (=nombre de lignes),
  # theta quantile souhaité
  
  xi1 <- Xi1(theta)
  xi2 <- Xi2(theta)
  p <- length(beta0)
  
  # simulation des variables explicatives x
  # S <- matrix(0, nrow=p, ncol=p)
  # for(k in 1:p){ for(l in 1:p){ S[k,l] <- 0.5**abs(k-l) } }
  # x <- matrix(0, nrow=n, ncol=p)
  # for(i in 1:n){ x[i,] <- mvtnorm::rmvnorm(1, mean=rep(0, p), sigma=S) }
  S <- outer(1:p, 1:p, function(a,b) 0.5**abs(a-b))
  X <- mvtnorm::rmvnorm(n, sigma=S)
  
  # simulation des erreurs
  if(g_err){
    ## erreurs gaussiennes :
    delta <- qnorm(theta, mean=0, sd=3)
    u <- rnorm(n, mean=-delta, sd=3)
  } else {
    ## erreurs du modèle quantile elastic net :
    v <- rexp(n)
    v_bar <- v / tau0
    z <- rnorm(n)
    u <- xi1 * v_bar + xi2 * sqrt(v_bar / tau0) * z
  }
  
  # calcul de y :
  y <- X %*% beta0 + u
  return( list(x=X, y=y) )
}
```

```{r, individual-updates}
#| echo: false

a <- 0.1
b <- 0.1
c1 <- 0.1
c2 <- 0.1
d1 <- 0.1
d2 <- 0.1

v_bar_update <- function(tau, beta, n, x, y, xi2, xi1){
  chi_v <- tau * (y - x%*%beta)**2 / (xi2** 2)
  psi_v <-(tau* xi1**2)/xi2**2 + 2*tau
  v_bar <- rep(0,n)
  # rgig can not deal with varying chi parameter
  for(i in 1:n){
    v_bar[i] <- rgig(1, lambda=1/2, chi=chi_v[i], psi=psi_v)
  }
  return(v_bar)
}

y_bar_update <- function(v_bar, beta, n, p, xi1, x, y){
  y_bar <- matrix(nrow=n, ncol=p)
  for(k in 1:p){
    y_bar[,k] <- y - xi1*v_bar - rowSums( t( t(x)*beta)[,-k] )
  }
  return(y_bar)
}

t_update <- function(beta, eta1_bar, eta2, p){
  chi_t <- 2 * eta2 * (beta**2)
  psi_t <- 2 * eta1_bar
  t <- rep(0, p)
  for(k in 1:p){
    t[k] <- rgig(1, lambda=1/2, chi=chi_t[k], psi=psi_t)
  }
  return(1 + t)
}

beta_update <- function(tau, v_bar, t, y_bar, eta2, xi2, x, y, p){
  sigma_inv2 <- tau * xi2 **(-2) * colSums((x**2) * v_bar**(-1)) + 2 * eta2 * t/(t - 1)
  sigma_bar2 <- 1 / sigma_inv2
  mu_bar     <- (sigma_bar2)* tau * xi2**(-2) * colSums( y_bar * x * (v_bar**(-1)) )
  beta       <- rnorm(p, mean=mu_bar, sd=sqrt(sigma_bar2))
  return(beta)
}

eta1_bar_update <- function(t, eta1_bar, p){
  # one step Metropolis Hastings
  # proposition pour new_eta1_bar :
  prop_eta1_bar <- rgamma(1, shape=c1 + p, rate=d1 + sum(t))
  # acceptation / rejet :
  d_eta1_bar <- function(eta){
    expint::gammainc(1/2,eta)^(-p) * eta^( p/2 + c1 - 1 ) * exp( -eta*(d1+sum(t)) )
  }
  alpha <- d_eta1_bar(prop_eta1_bar) / d_eta1_bar(eta1_bar)
  #   dgamma(
  #   prop_eta1_bar,
  #   shape=c1 + p,
  #   rate=d1 + sum(t-1)
  # ) / dgamma(
  #   eta1_bar,
  #   shape=c1 + p,
  #   rate=d1 + sum(t-1)
  # )
  # Arthur: c'est normal de ne jamais utiliser
  # la vraie loi de eta1_bar ?
  u <- runif(1)
  if(u < alpha){
    return(prop_eta1_bar)
  } else {
    return(eta1_bar)
  }
}

eta2_update <- function(beta, t, p){
  eta2 <- rgamma( 1, shape=c2 + p/2, rate=d2 + sum( (beta**2)*t/(t-1) ) )
  return(eta2)
}

tau_update <- function(v_bar, beta, x, y, xi1, xi2, n){
  r <- b + sum( v_bar + (y - x%*%beta - xi1*v_bar)**2 / (2*v_bar*xi2**2) )
  tau <- rgamma( 1, shape=a + 3*n/2, rate= r)
  return(tau)
}
```

```{r, general-update}
#| echo: false
Gibbs_update <- function(tau, eta1_bar, eta2, beta, x, y, theta){
  
  # met à jours les paramètres tau, eta1_bar, eta2 et beta
  # après une étape de Gibbs
  
  n <- length(y)
  p <- length(beta)
  xi1 <- Xi1(theta)
  xi2 <- Xi2(theta)

  v_bar <- v_bar_update(tau, beta, n, x, y, xi2, xi1)
  y_bar <- y_bar_update(v_bar, beta, n, p, xi1, x, y)
  t <- t_update(beta, eta1_bar, eta2, p)
  new_beta <- beta_update(tau, v_bar, t, y_bar, eta2, xi2, x, y, p)
  new_eta1_bar <- eta1_bar_update(t, eta1_bar, p)
  new_eta2 <- eta2_update(new_beta, t, p)
  new_tau <- tau_update(v_bar, new_beta, x, y, xi1, xi2, n)

  return( c(new_tau, new_eta1_bar, new_eta2, new_beta) )
}
```

```{r, simulation-function}
#| echo: false

# pour un échantillon iid prendre
# burnin = 10
# autocorr = 10

Generation_Gibbs <- function(r_gibbs, x, y, theta, burnin=0, autocorr=0){
  
  # génère un tableau avec r_gibbs échantillonages de Gibbs des variables :
  # tau : première colonne
  # eta1_bar : deuxième colonne
  # eta2 : troisième colonne
  # beta : colonnes restantes
  
  # initialisation des paramètres
  tau <- rgamma(1, shape=a, rate=b)
  eta1_bar <- rgamma(1, shape=c1, rate=d1)
  eta2 <- rgamma(1, shape=c2, rate=d2)
  # eta1_bar <- (eta1**2) / (4*eta2)
  beta <- rep(1, length(x[1,]))
  
  # tables vides
  tau_gibbs <- matrix(nrow=r_gibbs, ncol=1)
  eta1_bar_gibbs <- matrix(nrow=r_gibbs, ncol=1)
  eta2_gibbs <- matrix(nrow=r_gibbs, ncol=1)
  beta_gibbs <- matrix(nrow=r_gibbs, ncol=length(x[1,]))
  
  # burn in :
  for(r in 1:burnin){
    res <- Gibbs_update(tau, eta1_bar, eta2, beta, x, y, theta)
    tau <- res[1]
    eta1_bar <- res[2]
    eta2 <- res[3]
    beta <- res[-c(1:3)]
  
  }
  
  # gibbs sampling :
  for(r in 1:r_gibbs){
    for(j in 1:(1+autocorr)){
      res <- Gibbs_update(tau, eta1_bar, eta2, beta, x, y, theta)
      tau <- res[1]
      eta1_bar <- res[2]
      eta2 <- res[3]
      beta <- res[-c(1:3)]
    }
    tau_gibbs[r] <- tau
    eta1_bar_gibbs[r] <- eta1_bar
    eta2_gibbs[r] <- eta2
    beta_gibbs[r,] <- beta
  }
  
  # cbind(tau_gibbs, eta1_bar_gibbs, eta2_gibbs, beta_gibbs)
  return(list(tau = tau_gibbs, eta1_bar = eta1_bar_gibbs, eta2 = eta2_gibbs, beta = beta_gibbs))
}
```

```{r, check-data}
#| echo: false
set.seed(200)
theta <- 0.5
data <- data_generation(100, theta, beta0=c(1,-3,0,5,0,2,0,10,-2,3), tau0 = 2)
y <- data$y
x <- data$x

r_gibbs <- 200
res <- Generation_Gibbs(r_gibbs, x, y, theta)
tau_gibbs <- res$tau
eta1_bar_gibbs <- res$eta1_bar
eta2_gibbs <- res$eta2
beta_gibbs <- res$beta
```

```{r, burnin-graph}
#| echo: false
#| message: false
#| warning: false
library(ggplot2)
library(dplyr)
burnin_beta <- beta_gibbs |>
    as_tibble(.name_repair = function(x) paste0("beta",1:length(x))) |>
  mutate(run=1:nrow(beta_gibbs)) |>
  tidyr::pivot_longer(cols=-run, values_to = "coefficient", names_to = "id", names_prefix = "beta") |>
  ggplot(aes(x=run, y=coefficient)) +
  geom_line(aes(group=id, colour=id)) +
  labs(
    subtitle=expression(paste("Values of ", beta, " per iteration")),
    y=expression(beta[k])
  )

burnin_tau <- tibble(tau=tau_gibbs) |>mutate(run=1:nrow(beta_gibbs)) |>
  mutate(run=1:nrow(beta_gibbs)) |>
  tidyr::pivot_longer(cols=-run, values_to = "coefficient", names_to = "id") |>
  ggplot(aes(x=run, y=coefficient)) +
  geom_line(aes(group=id, colour=id)) +
  labs(
    subtitle=expression(paste("Values of ", tau, " per iteration")),
    y=expression(tau)
  )
  

burnin_eta <- tibble(eta1_tilde=eta1_bar_gibbs, eta2=eta2_gibbs) |>
  mutate(run=1:nrow(beta_gibbs)) |>
  tidyr::pivot_longer(cols=-run, values_to = "coefficient", names_to = "id", names_prefix = "beta") |>
  ggplot(aes(x=run, y=coefficient)) +
  geom_line(aes(group=id, colour=id)) +
  labs(
    subtitle=expression(paste("Values of ", tilde(eta)[1], " and ", eta[2]," per iteration")),
    y=expression(paste(tilde(eta)[1], " , ", eta[2]))
  )

library(patchwork)
(
  (burnin_beta | (burnin_tau / burnin_eta)) &
    guides(color='none') &
    labs(x=NULL)
)+
  plot_annotation(title = "Burnin is over after 10-20 iterations (n=100, d=10)")
```

```{r}
#| echo: false
#| message: false
#| warning: false
library(forecast)
( ( (
  (ggAcf(beta_gibbs[,1]) + labs(y=expression(beta[1])) ) /
  (ggAcf(beta_gibbs[,2])  + labs(y=expression(beta[2])) ) /
  (ggAcf(beta_gibbs[,3])  + labs(y=expression(beta[3])) ) /
  (ggAcf(beta_gibbs[,4])  + labs(y=expression(beta[4])) )
) | (
  (ggAcf(tau_gibbs)     + labs(y=expression(tau)) )/
  (ggAcf(eta1_bar_gibbs)+ labs(y=expression(tilde(eta)[1])) )/
  (ggAcf(eta2_gibbs)    + labs(y=expression(eta[2])) )
) ) & labs(title=NULL, x=NULL)
) +
  plot_annotation(
    title = "Autocorrelation fades out after 15-20 runs (n=100, d=10)",
    subtitle = "Autocorrelation function for increasing lags"
  )
  
```

In the graphs above and in the results hereafter, we simulate data from $\mathbf{x} \sim N(0, \Sigma)$ where $\Sigma_{k, l} = 0.5^{|i\k-l|}$ and choose $\boldsymbol{\beta}=(1,-3,0,5,0,2,0,10,-2,3)$. We purposely choose three zero-coefficients and a small number of observations (n=100), a setting where Elasticnet is supposedly more efficient than classical least-square regression. In the results section, we use $\theta=0.3$, we test two specifications for the error term $u$:

- Skewed Laplace error with shape parameter $\tau$ and whose $\theta$-th quantile is equal to $0$. These errors correspond to what is assumed in the model developed in the paper[^10]. 
- Gaussian errors with standard deviation $3$ and whose $\theta$-th quantile is equal to $0$. These errors are used in the article and enable the evaluation of the model when it is misspecified.

[^10]: Weirdly enough, the Skewed Laplace error is not evaluated in the reference article. The authors justify that this prior is too specific and that they prefer to test for robustness.

In the result section below, we also compare Bayesian and non-Baysian quantile Elasticnet regressions, using the library `hqreg` developped by @hqreg. This library gives an estimator of $\boldsymbol{\beta}$ for the regularised quantile regression task with the following Elasticnet penalty using $(\alpha,\lambda)$ instead of our specification $(\lambda_1,\lambda_2)$ : $$\textrm{penalty} =  \lambda \alpha \|\boldsymbol{\beta}\|_1 + \frac{\lambda (1-\alpha)}{2} \|\boldsymbol{\beta}\|_2^2$$

The library furnishes cross-validation over $\lambda$ with mean square error. We further perform cross-validation over $\alpha$ to obtain a single output for $\beta$ estimator.

In our comparisons, $\hat{\boldsymbol{\beta}}_n$ is the classical estimator for the non-Bayesian version. It is the empirical average over 100 generations of $\boldsymbol{\beta}\mid \mathbf{y},\mathbf{X}$ in the Bayesian case.

## Results

Globally, results from the Bayesian version (ours) underperfom the ones from the frequentist version (`hqreg`'s). However, it is much faster, in the magnitude order of seconds vs minutes.

| $\theta$ | Error | Version| Sample size | Exec. time | Loss | $\|\hat\beta_n-\beta_0\|_2^2$  |
|-|-|-|-:|-:|-|-|
| 0.3 | Normal | Bayesian |  50 | s | 
| 0.3 | Normal | Bayesian | 500 | m
| 0.3 | Normal | Bayesian |
| 0.3 | Normal | Bayesian |
| 0.3 | Laplace | Non-Bayesian | |

We find that skewed-Laplace errors give better results than Normal errors, which is expected.

In conclusion, we have reproduced the Bayesian quantile Elasticnet regression proposed in @2010-quantile-regression and have extended their experiments. We cannot confirm their findings that their methods outperform both the non-Bayesian estimates, but nevertheless both estimates remain close to each other. However, the 10-fold gain in execution time may eventually favour the Bayesian method, especially for small sample sizes.








RQ générales : 


peut être est ce du au calcul de lambda1 et lambda2 -> couple (l1, l2) associé à 1 estimateur beta, est ce que le couple moyen (l1_bar, l2_bar) associé au beta moyen ?

résultats :
 paramètres 
n_samples_test = 100
theta = 0.3
beta0 = c(rep(3,15), rep(0, 15))
tau0 = 2

g_err = FALSE
n_samples_train = 50
 méthode classique : 1min 55s d'execution, loss = 1.34, $\| \hat{\beta} - \beta0 \|^2 = 2.5$
 méthode bayésienne : (moyenne sur 100 valeurs) 6s exec, loss = 179  (Rq la loss ne dépend pas vraiment du nb d'échantillons générés avec Gibbs)  $\| \hat{\beta} - \beta0 \|^2 = 16$

g_err = FALSE
n_samples_train = 500
  méthode classique : 14s d'execution, loss = 0.77, $\| \hat{\beta} - \beta0 \|^2 = 0.13$
  méthode bayésienne : (moyenne sur 100 valeurs) 30s, loss = 14 , $\| \hat{\beta} - \beta0 \|^2 = 0.23$
  
erreur gaussienne
n_samples_train = 50
 méthode classique : 2min 24s d'execution, loss = 2.11, $\| \hat{\beta} - \beta0 \|^2 = 11$
 méthode bayésienne : (moyenne sur 100 valeurs) 7s exec, loss = 157  $\| \hat{\beta} - \beta0 \|^2 = 19$
 
erreur gaussienne
n_samples_train = 500
 méthode classique : 14s d'execution, loss = 1.8, $\| \hat{\beta} - \beta0 \|^2 = 0.6$
 méthode bayésienne : (moyenne sur 100 valeurs) 27s exec, loss = 31  $\| \hat{\beta} - \beta0 \|^2 = 0.9$

## Appendix

```r

```

### Project checks

From the project instructions

-   [ ] report as a pdf named SurnameStudent1_SurnameStudent2,
-   [ ] a zipped folder SurnameStudent1_SurnameStudent2 containing your code and a detailed readme file with instructions to run the code
