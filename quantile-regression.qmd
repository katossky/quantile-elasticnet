---
title: "A partial reproduction of \"Bayesian regularised quantile regression\""
authors: "Lucie Tournier & Arthur Katossky"
date: "December 2022"
date-format: "MMMM, YYYY"
bibliography: quantile-regression.bib
---

This reports reproduces the results @2010-quantile-regression, focussing on their extension of the quantile regression with Elasticnet regularisation in a Bayesian framework. Regression is the idea that, for an outcome $y\in\mathbb{R}$ and associated inputs $\mathbf{x}\in\mathbb{R}^d$ ($d \in \mathbb{N}_\star$), we may suppose that in some sense $y\simeq \mathbf{x}^\top\boldsymbol{\beta}$. Given a set of observations $(y_i,\mathbf{x}_i)_{i=1..n}$ classical regression estimates the unknown parameter $\boldsymbol{\beta}$ by setting: $$\hat{\boldsymbol{\beta}}_n=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^d}\sum_{i=1}^n \ell(y_i, \mathbf{x}_i^\top\boldsymbol{\beta})$${#eq-classical-regression} ... where $\ell$ is some loss function, typically $\ell_2(a,b)=(a-b)^2$ for the ordinary least squares. In a **quantile regression**, one rather opts for the asymmetric "check loss" : $$\ell_\theta(a,b)=\left\{\begin{array}{cl}\theta (a-b), & \text { if } a \geqslant b \\ -(1-\theta) (a-b), & \text { if }  a<b\end{array}\right.$$ ... with $\theta\in(0,1)$, which generalises absolute loss $\ell_0(a,b)=|a-b|$ leading to so-called median regression. (Note that $\ell_0$ is proportional to $\ell_\theta$ with $\theta=0.5$ and that the proportionality coefficient does not matter in @eq-classical-regression, so the two losses are completely interchangeable.) Next, Elasticnet regularisation consists into constraining the values of $\boldsymbol{\beta}$ not to move too far away from the origin, in the sense that both $\|\boldsymbol{\beta}\|_1$ (the number of non-zero components) and $\|\boldsymbol{\beta}\|_2^2$ (the square of the distance to the origin) are small. This leads to the final problem: $$\hat{\boldsymbol{\beta}}_n(\lambda_1,\lambda_2,\theta)=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^d}\sum_{i=1}^n \ell_\theta(y_i, \mathbf{x}_i^\top\boldsymbol{\beta})+\lambda_1\|\boldsymbol{\beta}\|_1+\lambda_2\|\boldsymbol{\beta}\|^2_2$$ ... where $\lambda_1$ and $\lambda_2$ control how stringent each regularisation is (less strigent as $\lambda \to 0$).

The article studies **how to put Elasticnet quantile regression into a Bayesian paradigm**, thus allowing for priors over the distributions of both $\boldsymbol{\beta}$ and the approximation error in $y\simeq \mathbf{x}^\top\boldsymbol{\beta}$.

## Putting regularised quantile regression into the Bayesian formalism (1 page)

> Definition of the problem under study and explanation of why it is interesting

## Distribution of interest parameter $\boldsymbol{\beta}$ may be elicited through Gibbs sampling (1,5 page)

> Choice of the appropriate Bayesian technique (you should explain the methodology used and the motivation why you have chosen it)

## Simulation (1,5 page)

> Description of the computational method used and difficulties encountered

## Results (1 page)

> Explanation and interpretation of the results.

## Appendices

### Code

### Project checks

From the project instructions

- [ ] The project length has to be about 5 pages (plus code)
- [ ] Any language among Python, $\mathrm{R}$ or Matlab
- [ ] Due date of the project is January 15,2023 . By the due date please upload
- [ ] report as a pdf named SurnameStudent1_SurnameStudent2,
- [ ] a zipped folder SurnameStudent1_SurnameStudent2 containing your code and a detailed readme file with instructions to run the code

### Arthur's notes

Focus on prediction accuracy (not on reconstruction validity or inference).

> we give a generic treatment to a set of regularization approaches, including lasso, group lasso and elastic net penalties.